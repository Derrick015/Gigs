---
title: "Stepwise and Lasso Regression"
output: html_document
---


# Presentation of theoretical bases

## Stepwise Regression

A stepwise regression is an iterative construction of a regression model that involving the selection of relevant independent variables to be utilized in a final model. This method examines the statistical significance of each independent variable in a linear regression model. This form of regression is mostly used when there are numerous independent variables that produces insignificant outcomes because of the small sample size or high correlation between independent variables.
Stepwise regression can be undertaken either by regressing each independent variable at a time and including it in the final regression model if it is statistically significant or selecting all the relevant independent variables and including them in the model, only to eliminate those that are not statistically significant. Alternatively an amalgamation of both can be applied. This creates three methods for the step wise regression:
.	Forward selection: Each independent variable is tested and included in the model if it is statistically significant. This process is repeated till all significant variables are captured
.	Backward elimination: Here we start with a group of independent variables, remove one at a time then test to see if the removed variable was statistically significant
.	Bidirectional elimination: This combines both forward selection and backward elimination to determine which independent variable is included or excluded from the final model.


##Lasso Regression

"LASSO" denotes Least Absolute Shrinkage and Selection Operator. Lasso regression is a regularization technique that is used over linear regression for better predictions.  It does this through shrinkage. This is where data values are shrunk towards a central point like the mean. 
With lasso regression a penalty term is introduced which penalizes the absolute size of the regression coefficients. By curtailing the sum of the absolute value of the estimates, some coefficients can become zero, thus removing them from the model. A bigger penalty will future cause more shrinkage towards zero. This is useful when you wish to select variables for a model and helps in situations where there are high levels of multicollinearity.
Mathematically the lasso regression equation can be represented by:
 

That is: Residual Sum of Squares + ?? * (Sum of the absolute value of the magnitude of coefficients)
.	Lambda (??) demotes the amount of shrinkage
.	When lambda is zero all variables are maintained and the estimate is equivalent to that of a linear regression.
.	As lambda increases more coefficients will be set to zero and in effect eliminate the feature or variable. Theoretically when lambda is equal to infinity all the coefficients will be removed.
.	As lambda increases bias increases
.	As lambda decreases the variance increases


# Presentation of database

```{r setup}
# Data importation
library(tidyverse)
train_set <- read.csv("./train.csv")
test_set <- read.csv("./test.csv")
```

```{r_dataset, echo=FALSE}
# Presentation of dataset
full_set<-bind_rows(train_set,test_set) # Binding the data and test set
str(full_set)
```

Analysis of the the database shows 2919 obervatoins and 81 variables with differing class types.
